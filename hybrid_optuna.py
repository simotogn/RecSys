# -*- coding: utf-8 -*-
"""Hybrid Optuna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oBzc4_N-WXZaOgfHOdlQJbnQ3VK4ZnfK
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/libs

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import scipy as sp
import scipy.sparse as sps
import matplotlib.pyplot as plt
import random
from scipy import stats
from scipy.optimize import fmin
import pandas as pd

from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample

data = pd.read_csv(filepath_or_buffer="data_train.csv",
                   sep=",")
data.columns = ["UserID", "ItemID", "Interaction"]

mapped_id, original_id = pd.factorize(data["UserID"].unique())
user_original_ID_to_index = pd.Series(mapped_id, index=original_id)

mapped_id, original_id = pd.factorize(data["ItemID"].unique())
item_original_ID_to_index = pd.Series(mapped_id, index=original_id)

data["UserID"] = data["UserID"].map(user_original_ID_to_index)
data["ItemID"] = data["ItemID"].map(item_original_ID_to_index)


URM_all = sps.coo_matrix((data["Interaction"].values,
                          (data["UserID"].values, data["ItemID"].values)))

URM_all.tocsr()

URM_train_validation, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)
URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train_validation, train_percentage = 0.8)

URM_all, URM_train_validation, URM_train, URM_validation, URM_test

from Recommenders.BaseRecommender import BaseRecommender

class ScoresHybridRecommender(BaseRecommender):
    """ ScoresHybridRecommender
    Hybrid of two prediction scores R = R1*alpha + R2*(1-alpha)

    """

    RECOMMENDER_NAME = "ScoresHybridRecommender"

    '''def __init__(self, URM_train, alpha_R, beta, topK_R, alpha_S, l1_ratio, topK_S):
        super(ScoresHybridRecommender, self).__init__(URM_train)

        self.URM_train = sps.csr_matrix(URM_train)
        self.recommender_1 = RP3betaRecommender(URM_train)
        self.recommender_1.fit(alpha = alpha_R, beta =beta, topK = topK_R,implicit = False, normalize_similarity = True)

        self.recommender_2 = SLIMElasticNetRecommender(URM_train)
        self.recommender_2.fit(alpha = alpha_S, l1_ratio = l1_ratio, topK = topK_S)'''

    def __init__(self, URM_train, recommender1, recommeder2):
          super(ScoresHybridRecommender, self).__init__(URM_train)

          self.URM_train = sps.csr_matrix(URM_train)
          self.recommender_1 = recommender1
          self.recommender_2 = recommeder2


    def fit(self, alpha = 0.5):
        self.alpha = alpha


    def _compute_item_score(self, user_id_array, items_to_compute):

        # In a simple extension this could be a loop over a list of pretrained recommender objects
        item_weights_1 = self.recommender_1._compute_item_score(user_id_array)
        item_weights_2 = self.recommender_2._compute_item_score(user_id_array)

        item_weights = item_weights_1*self.alpha + item_weights_2*(1-self.alpha)

        return item_weights

from Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender

RP3beta = RP3betaRecommender(URM_train)
RP3beta.fit(topK= 27, alpha= 0.3394421657218524, beta= 0.1740360250444788, implicit = False, normalize_similarity = True )

from Recommenders.SLIM.SLIMElasticNetRecommender import SLIMElasticNetRecommender

SLIM_MSE = SLIMElasticNetRecommender(URM_train)
SLIM_MSE.fit(topK= 206, alpha= 0.008035973911931452, l1_ratio= 4.977545006991767e-05)

"""#Try Hyperparameter tuning with Optuna"""

pip install optuna

from Evaluation.Evaluator import EvaluatorHoldout


evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])
evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])

import optuna
import pandas as pd
from Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender
from Recommenders.SLIM.SLIMElasticNetRecommender import SLIMElasticNetRecommender

#"alpha": Real(0.1,0.2),    "beta" : Real(0.1,0.2),    "topK" : Integer(10,50)


def objective_function(optuna_trial):


    recommender_instance = ScoresHybridRecommender(URM_train, RP3beta, SLIM_MSE
                                                   #alpha_S = optuna_trial.suggest_float("alpha_S", 0.001, 1.0),
                                                   #alpha_R = optuna_trial.suggest_float("alpha_R", 0.001, 1.0),
                                                   #beta = optuna_trial.suggest_float("beta", 0.001, 1.0),
                                                   #topK_S = optuna_trial.suggest_int("topK_S", 5, 300),
                                                   #topK_R = optuna_trial.suggest_int("topK_R", 5, 100),
                                                   #l1_ratio = optuna_trial.suggest_float("l1_ratio", 1e-6, 1e-4),
                                                   )
    recommender_instance.fit(alpha = optuna_trial.suggest_float("alpha", 0.001, 1.0),
                            )

    result_df, _ = evaluator_validation.evaluateRecommender(recommender_instance)

    return result_df.loc[10]["MAP"]

class SaveResults(object):

    def __init__(self):
        self.results_df = pd.DataFrame(columns = ["result"])

    def __call__(self, optuna_study, optuna_trial):
        hyperparam_dict = optuna_trial.params.copy()
        hyperparam_dict["result"] = optuna_trial.values[0]

        self.results_df = self.results_df.append(hyperparam_dict, ignore_index=True)

optuna_study = optuna.create_study(direction="maximize")

save_results = SaveResults()

optuna_study.optimize(objective_function,
                      callbacks=[save_results],
                      n_trials = 100)

pruned_trials = [t for t in optuna_study.trials if t.state == optuna.trial.TrialState.PRUNED]
complete_trials = [t for t in optuna_study.trials if t.state == optuna.trial.TrialState.COMPLETE]

print("Study statistics: ")
print("  Number of finished trials: ", len(optuna_study.trials))
print("  Number of pruned trials: ", len(pruned_trials))
print("  Number of complete trials: ", len(complete_trials))

print("Best trial:")
print("  Value Validation: ", optuna_study.best_trial.value)

optuna_study.best_trial

optuna_study.best_trial.params

save_results.results_df

recommender_instance = (URM_train + URM_validation)
recommender_instance.fit(**optuna_study.best_trial.params)

result_df, _ = evaluator_test.evaluateRecommender(recommender_instance)
result_df

"""#Let's submit

RP3beta.fit(alpha = 0.30045048565736454, beta=0.15900639335692543, topK=34, implicit = False, normalize_similarity = True) + SLIM_MSE.fit(alpha = 0.0033868445794621033, l1_ratio=2.1304972208263965e-05, topK=36) + alpha = 0.5042400731098166. val:3.09473% submit 13.62%

RP3beta.fit(alpha = 0.30045048565736454, beta=0.15900639335692543, topK=34, implicit = False, normalize_similarity = True) + SLIM_MSE.fit(alpha = 0.0026695828861131796, l1_ratio=9.7058223865738e-05, topK=237) + alpha:0.42280199553592035 val: 3.09616% submit:13.801%

RP3beta.fit(alpha = 0.3388784009010535, beta=0.177797903368052, topK=30, implicit = False, normalize_similarity = True) + SLIM_MSE.fit(alpha = 0.0026695828861131796, l1_ratio=9.7058223865738e-05, topK=237) + alpha = 0.5616855740532924 val: 3.082% submit: 13.766%

RP3beta.fit(alpha = 0.30045048565736454, beta=0.15900639335692543, topK=34, implicit = False, normalize_similarity = True) +
SLIM_MSE.fit(alpha = 0.0030080355721395845, l1_ratio=5.495490764642386e-06, topK=207) +  alpha:0.4451229997552284 + val:3.034% submit:13.8%

RP3beta.fit(alpha = 0.32572640698412575, beta=0.18217576354665624, topK=31, implicit = False, normalize_similarity = True) + SLIM_MSE.fit(alpha = 0.0026695828861131796, l1_ratio=9.7058223865738e-05, topK=237) + alpha:0.4420834976325743 val3.146% submit: 13.82%

RP3beta.fit(alpha = 0.31583032356728813, beta=0.1936831182299878, topK=29, implicit = False, normalize_similarity = True ) + SLIM_MSE.fit(alpha = 0.0026695828861131796, l1_ratio=9.7058223865738e-05, topK=237) + alpha:0.4360593316742256 val:3.0834% submit: 13.82%

RP3beta.fit(alpha = 0.31583032356728813, beta=0.1936831182299878, topK=29, implicit = False, normalize_similarity = True ) + SLIM_MSE.fit(topK= 238, alpha= 0.046716458569833155, l1_ratio= 5.309133400047306e-05) + alpha = 0.5053704278742569 + val: 3.083% submit: 13.023%

RP3beta.fit(topK= 27, alpha= 0.3394421657218524, beta= 0.1740360250444788, implicit = False, normalize_similarity = True ) + SLIM_MSE.fit(topK= 206, alpha= 0.008035973911931452, l1_ratio= 4.977545006991767e-05)
"""

from Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender

RP3beta = RP3betaRecommender(URM_all)
RP3beta.fit(alpha = 0.31583032356728813, beta=0.1936831182299878, topK=29, implicit = False, normalize_similarity = True )

from Recommenders.SLIM.SLIMElasticNetRecommender import SLIMElasticNetRecommender

SLIM_MSE = SLIMElasticNetRecommender(URM_all)
SLIM_MSE.fit(topK= 238, alpha= 0.046716458569833155, l1_ratio= 5.309133400047306e-05)

from Recommenders.GraphBased.RP3betaRecommender import RP3betaRecommender
from Recommenders.SLIM.SLIMElasticNetRecommender import SLIMElasticNetRecommender

#{'alpha_S': 0.0018039635745485064, 'alpha_R': 0.0034513779391265736, 'beta': 0.9111983209661509, 'topK_S': 187, 'topK_R': 93, 'l1_ratio': 9.371084323801284e-05, 'alpha': 0.2838202981991156}.

recommender = ScoresHybridRecommender(URM_all, RP3beta, SLIM_MSE)
recommender.fit(0.5053704278742569)

users_to_recommend_raw = pd.read_csv(filepath_or_buffer="/gdrive/My Drive/libs/data_target_users_test.csv",
                   sep=",",
                  dtype={"user_id": np.int32}).to_numpy()
users_to_recommend = []
for elem in users_to_recommend_raw:
  users_to_recommend.append(elem[0])

len(users_to_recommend)

ratings = pd.read_csv("/gdrive/My Drive/libs/data_train2.csv",
                       sep=",",
                       names=["user_id", "item_id", "ratings"],
                       header=None,
                       dtype={"user_id": np.int64,
                               "item_id": np.int64,
                               "ratings": np.int64})
ratings.shape

def preprocess_data(ratings: pd.DataFrame):
    unique_users = ratings.user_id.unique()
    unique_items = ratings.item_id.unique()

    num_users, min_user_id, max_user_id = unique_users.size, unique_users.min(), unique_users.max()
    num_items, min_item_id, max_item_id = unique_items.size, unique_items.min(), unique_items.max()

    print(num_users, min_user_id, max_user_id)
    print(num_items, min_item_id, max_item_id)

    mapping_user_id = pd.DataFrame({"mapped_user_id": np.arange(num_users), "user_id": unique_users})
    mapping_item_id = pd.DataFrame({"mapped_item_id": np.arange(num_items), "item_id": unique_items})

    ratings = pd.merge(left=ratings,
                       right=mapping_user_id,
                       how="inner",
                       on="user_id")

    ratings = pd.merge(left=ratings,
                       right=mapping_item_id,
                       how="inner",
                       on="item_id")

    return ratings

ratings = preprocess_data(ratings)

users_ids_and_mappings = ratings[ratings.user_id.isin(users_to_recommend)][["user_id", "mapped_user_id"]].drop_duplicates()
items_ids_and_mappings = ratings[["item_id", "mapped_item_id"]].drop_duplicates()
users_ids_and_mappings

def prepare_submission(ratings: pd.DataFrame, users_to_recommend: np.array, urm_train: sps.csr_matrix):
    users_ids_and_mappings = ratings[ratings.user_id.isin(users_to_recommend)][["user_id", "mapped_user_id"]].drop_duplicates()
    items_ids_and_mappings = ratings[["item_id", "mapped_item_id"]].drop_duplicates()

    mapping_to_item_id = dict(zip(ratings.mapped_item_id, ratings.item_id))


    recommendation_length = 10
    submission = []
    for idx, row in users_ids_and_mappings.iterrows():
        user_id = row.user_id
        mapped_user_id = row.mapped_user_id
        recommendations = recommender.recommend(mapped_user_id,recommendation_length)
        submission.append((user_id, [mapping_to_item_id[item_id] for item_id in recommendations]))

    users_recommended = [elem[0] for elem in submission]

    missing_users = []

    top10 = [2, 4, 1, 7, 3, 6, 8, 9, 15, 20]  #con remove seen = False
    for elem in users_to_recommend:
      if elem not in users_recommended:
        missing_users.append(elem)

    for i in range(len(missing_users)):
      missing_users[i] = (missing_users[i], [mapping_to_item_id[top10[elem]] for elem in range(10)])

    return submission, missing_users

URM_train = URM_all.tocsr()
submission, missing_users = prepare_submission(ratings, users_to_recommend, URM_all)
len(submission), len(missing_users)

submission = submission + missing_users
len(submission)

submission.sort()

def write_submission(submissions):
    with open("./Hybrid_13.csv", "w") as f:
        f.write("user_id,item_list\n")
        for user_id, items in submissions:
          f.write(f"{user_id},{' '.join(str(item) for item in items)}\n")

write_submission(submission)

