# -*- coding: utf-8 -*-
"""Forth notebook Funk SVD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FXq1v9NXWn3NvMyMYuH70LlE8HSKY8ir
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/RecSys/libs

# Commented out IPython magic to ensure Python compatibility.
# %load_ext Cython

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import scipy as sp
import scipy.sparse as sps
import matplotlib.pyplot as plt
import random
from scipy import stats
from scipy.optimize import fmin
import pandas as pd

from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample

data = pd.read_csv(filepath_or_buffer="/gdrive/My Drive/RecSys/libs/data_train.csv",
                   sep=",")
data.columns = ["UserID", "ItemID", "Interaction"]

mapped_id, original_id = pd.factorize(data["UserID"].unique())
user_original_ID_to_index = pd.Series(mapped_id, index=original_id)

mapped_id, original_id = pd.factorize(data["ItemID"].unique())
item_original_ID_to_index = pd.Series(mapped_id, index=original_id)

data["UserID"] = data["UserID"].map(user_original_ID_to_index)
data["ItemID"] = data["ItemID"].map(item_original_ID_to_index)


URM_all = sps.coo_matrix((data["Interaction"].values,
                          (data["UserID"].values, data["ItemID"].values)))

URM_all.tocsr()

#URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)

#from sklearn.model_selection import train_test_split

#URM_train, URM_test = train_test_split(URM_all, random_state=42, test_size=0.1)

#URM_train.shape, URM_test.shape
URM_train = URM_all.tocsr()

n_users, n_items = URM_train.shape
n_users, n_items

# Commented out IPython magic to ensure Python compatibility.
# %%cython
# import numpy as np
# import time
# 
# from libc.stdlib cimport rand, srand, RAND_MAX
# 
# def train_multiple_epochs(URM_train, learning_rate_input, regularization_input, n_epochs):
# 
#     URM_train_coo = URM_train.tocoo()
#     n_users, n_items = URM_train_coo.shape
#     cdef int n_interactions = URM_train.nnz
# 
#     cdef int sample_num, sample_index, user_id, item_id, factor_index
#     cdef double rating, predicted_rating, prediction_error
# 
#     cdef int num_factors = 10
#     cdef double learning_rate = learning_rate_input
#     cdef double regularization = regularization_input
# 
#     cdef int[:] URM_train_coo_row = URM_train_coo.row
#     cdef int[:] URM_train_coo_col = URM_train_coo.col
#     cdef double[:] URM_train_coo_data = URM_train_coo.data
# 
#     cdef double[:,:] user_factors = np.random.random((n_users, num_factors))
#     cdef double[:,:] item_factors = np.random.random((n_items, num_factors))
#     cdef double H_i, W_u
#     cdef double item_factors_update, user_factors_update
# 
#     cdef double loss = 0.0
#     cdef long start_time = time.time()
# 
#     for n_epoch in range(n_epochs):
# 
#         loss = 0.0
#         start_time = time.time()
# 
#         for sample_num in range(URM_train.nnz):
# 
#             # Randomly pick sample
#             sample_index = rand() % n_interactions
# 
#             user_id = URM_train_coo_row[sample_index]
#             item_id = URM_train_coo_col[sample_index]
#             rating = URM_train_coo_data[sample_index]
# 
#             # Compute prediction
#             predicted_rating = 0.0
# 
#             for factor_index in range(num_factors):
#                 predicted_rating += user_factors[user_id, factor_index] * item_factors[item_id, factor_index]
# 
#             # Compute prediction error, or gradient
#             prediction_error = rating - predicted_rating
#             loss += prediction_error**2
# 
#             # Copy original value to avoid messing up the updates
#             for factor_index in range(num_factors):
# 
#                 H_i = item_factors[item_id,factor_index]
#                 W_u = user_factors[user_id,factor_index]
# 
#                 user_factors_update = prediction_error * H_i - regularization * W_u
#                 item_factors_update = prediction_error * W_u - regularization * H_i
# 
#                 user_factors[user_id,factor_index] += learning_rate * user_factors_update
#                 item_factors[item_id,factor_index] += learning_rate * item_factors_update
# 
#         elapsed_time = time.time() - start_time
#         samples_per_second = sample_num/elapsed_time
# 
#         print("Epoch {} complete in in {:.2f} seconds, loss is {:.3E}. Samples per second {:.2f}".format(n_epoch+1, time.time() - start_time, loss/sample_num, samples_per_second))
# 
#     return np.array(user_factors), np.array(item_factors), loss, samples_per_second

n_items = URM_train.shape[1]
learning_rate = 1e-3
regularization = 1e-5

user_factors, item_factors, loss, samples_per_second =  train_multiple_epochs(URM_train, learning_rate, regularization, 20)

user_id = 1
item_id = 10
predicted_rating = np.dot(user_factors[user_id,:], item_factors[item_id,:])
predicted_rating

URM_train[1]

def filter_seen(URM, user_id, scores):

        start_pos = URM.indptr[user_id]
        end_pos = URM.indptr[user_id+1]

        user_profile = URM.indices[start_pos:end_pos]

        scores[user_profile] = -np.inf

        return scores

def recommend(URM, user_factors, item_factors,  user_id, at=10, exclude_seen=True):
        # compute the scores using the dot product
        #user_profile = URM[user_id]
        scores = np.zeros(22222)

        for i in range(22222):
          scores[i] = np.dot(user_factors[user_id,:], item_factors[i,:])
        if exclude_seen:
            scores = filter_seen(URM, user_id, scores)

        # rank items
        ranking = scores.argsort()[::-1]

        return ranking[:at]

recommender = recommend(URM_train, user_factors, item_factors,1,10)
recommender

"""##****SUBMIT****"""

users_to_recommend_raw = pd.read_csv(filepath_or_buffer="/gdrive/My Drive/RecSys/libs/data_target_users_test.csv",
                   sep=",",
                  dtype={"user_id": np.int32}).to_numpy()
users_to_recommend = []
for elem in users_to_recommend_raw:
  users_to_recommend.append(elem[0])

len(users_to_recommend)

ratings = pd.read_csv("/gdrive/My Drive/RecSys/libs/data_train2.csv",
                       sep=",",
                       names=["user_id", "item_id", "ratings"],
                       header=None,
                       dtype={"user_id": np.int64,
                               "item_id": np.int64,
                               "ratings": np.int64})
ratings.shape

def preprocess_data(ratings: pd.DataFrame):
    unique_users = ratings.user_id.unique()
    unique_items = ratings.item_id.unique()

    num_users, min_user_id, max_user_id = unique_users.size, unique_users.min(), unique_users.max()
    num_items, min_item_id, max_item_id = unique_items.size, unique_items.min(), unique_items.max()

    print(num_users, min_user_id, max_user_id)
    print(num_items, min_item_id, max_item_id)

    mapping_user_id = pd.DataFrame({"mapped_user_id": np.arange(num_users), "user_id": unique_users})
    mapping_item_id = pd.DataFrame({"mapped_item_id": np.arange(num_items), "item_id": unique_items})

    ratings = pd.merge(left=ratings,
                       right=mapping_user_id,
                       how="inner",
                       on="user_id")

    ratings = pd.merge(left=ratings,
                       right=mapping_item_id,
                       how="inner",
                       on="item_id")

    return ratings

ratings = preprocess_data(ratings)

ratings.user_id

users_ids_and_mappings = ratings[ratings.user_id.isin(users_to_recommend)][["user_id", "mapped_user_id"]].drop_duplicates()
items_ids_and_mappings = ratings[["item_id", "mapped_item_id"]].drop_duplicates()
users_ids_and_mappings

def prepare_submission(ratings: pd.DataFrame, users_to_recommend: np.array, urm_train: sps.csr_matrix, recommender: object):
    users_ids_and_mappings = ratings[ratings.user_id.isin(users_to_recommend)][["user_id", "mapped_user_id"]].drop_duplicates()
    items_ids_and_mappings = ratings[["item_id", "mapped_item_id"]].drop_duplicates()

    mapping_to_item_id = dict(zip(ratings.mapped_item_id, ratings.item_id))


    recommendation_length = 10
    submission = []
    for idx, row in users_ids_and_mappings.iterrows():
        user_id = row.user_id
        mapped_user_id = row.mapped_user_id
        #recommendations = []
        recommendations = recommend(URM_train,user_factors,item_factors, mapped_user_id,recommendation_length)
        submission.append((user_id, [mapping_to_item_id[item_id] for item_id in recommendations]))

    users_recommended = [elem[0] for elem in submission]

    missing_users = []
    top10 = [ 517,  189,   44,    0,  284,  808,  285,  557,    1, 1266]
    for elem in users_to_recommend:
      if elem not in users_recommended:
        missing_users.append(elem)

    for i in range(len(missing_users)):
      missing_users[i] = (missing_users[i], [mapping_to_item_id[top10[elem]] for elem in range(10)])

    return submission, missing_users

recommender = recommend(URM_train, user_factors,item_factors,user_id,10)
submission, missing_users = prepare_submission(ratings, users_to_recommend, URM_train, recommender)
len(submission), len(missing_users)

for i in range(10):
  print(submission[i])

for i in range(10):
  print(missing_users[i])

submission = submission + missing_users
len(submission)

submission.sort()
for i in range(len(submission)):
  submission[i][1].sort()
submission

def write_submission(submissions):
    with open("./FUNKSVDsubmission.csv", "w") as f:
        for user_id, items in submissions:
          f.write(f"{user_id},{' '.join(str(item) for item in items)}\n")

write_submission(submission)