# -*- coding: utf-8 -*-
"""Third notebook SLIM MSE with Gradient Descent and Cython.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AlD7-VIsqHGE3ZE3zNdcrBgjmD6yKrcy
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/RecSys/libs

# Commented out IPython magic to ensure Python compatibility.
# %load_ext Cython

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import scipy as sp
import scipy.sparse as sps
import matplotlib.pyplot as plt
import random
from scipy import stats
from scipy.optimize import fmin
import pandas as pd

from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample

data = pd.read_csv(filepath_or_buffer="/gdrive/My Drive/RecSys/libs/data_train.csv",
                   sep=",")
data.columns = ["UserID", "ItemID", "Interaction"]

mapped_id, original_id = pd.factorize(data["UserID"].unique())
user_original_ID_to_index = pd.Series(mapped_id, index=original_id)

mapped_id, original_id = pd.factorize(data["ItemID"].unique())
item_original_ID_to_index = pd.Series(mapped_id, index=original_id)

data["UserID"] = data["UserID"].map(user_original_ID_to_index)
data["ItemID"] = data["ItemID"].map(item_original_ID_to_index)


URM_all = sps.coo_matrix((data["Interaction"].values,
                          (data["UserID"].values, data["ItemID"].values)))

URM_all.tocsr()

#URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)

#from sklearn.model_selection import train_test_split

#URM_train, URM_test = train_test_split(URM_all, random_state=42, test_size=0.1)

#URM_train.shape, URM_test.shape
URM_train = URM_all.tocsr()

n_users, n_items = URM_train.shape
n_users, n_items

# Commented out IPython magic to ensure Python compatibility.
# %%cython
# 
# import numpy as np
# import time
# 
# from libc.stdlib cimport rand, srand, RAND_MAX
# 
# def train_multiple_epochs(URM_train, learning_rate_input, regularization_2_input, n_epochs):
# 
#     URM_train_coo = URM_train.tocoo()
#     cdef int n_items = URM_train.shape[1]
#     cdef int n_interactions = URM_train.nnz
#     cdef int[:] URM_train_coo_row = URM_train_coo.row
#     cdef int[:] URM_train_coo_col = URM_train_coo.col
#     cdef double[:] URM_train_coo_data = URM_train_coo.data
#     cdef int[:] URM_train_indices = URM_train.indices
#     cdef int[:] URM_train_indptr = URM_train.indptr
#     cdef double[:] URM_train_data = URM_train.data
# 
#     cdef double[:,:] item_item_S = np.zeros((n_items, n_items), dtype = float)
#     cdef double learning_rate = learning_rate_input
#     cdef double regularization_2 = regularization_2_input
#     cdef double loss = 0.0
#     cdef long start_time
#     cdef double true_rating, predicted_rating, prediction_error, profile_rating
#     cdef int start_profile, end_profile
#     cdef int index, sample_num, user_id, item_id, profile_item_id
# 
#     for n_epoch in range(n_epochs):
# 
#         loss = 0.0
#         start_time = time.time()
# 
#         for sample_num in range(n_interactions):
# 
#             # Randomly pick sample
#             index = rand() % n_interactions
# 
#             user_id = URM_train_coo_row[index]
#             item_id = URM_train_coo_col[index]
#             true_rating = URM_train_coo_data[index]
# 
#             # Compute prediction
#             start_profile = URM_train_indptr[user_id]
#             end_profile = URM_train_indptr[user_id+1]
#             predicted_rating = 0.0
# 
#             for index in range(start_profile, end_profile):
#                 profile_item_id = URM_train_indices[index]
#                 profile_rating = URM_train_data[index]
#                 predicted_rating += item_item_S[profile_item_id,item_id] * profile_rating
# 
#             # Compute prediction error, or gradient
#             prediction_error = true_rating - predicted_rating
#             loss += prediction_error**2
# 
#             # Update model, in this case the similarity
#             for index in range(start_profile, end_profile):
#                 profile_item_id = URM_train_indices[index]
#                 profile_rating = URM_train_data[index]
#                 item_item_S[profile_item_id,item_id] += learning_rate * (prediction_error * profile_rating -
#                                                                          regularization_2 * item_item_S[profile_item_id,item_id])
# 
#             # Ensure diagonal is always zero
#             item_item_S[item_id,item_id] = 0.0
# 
# #             if sample_num % 1000000 == 0:
# #                 print("Epoch {}: {:.2f}%".format(n_epoch+1, sample_num/n_interactions*100))
# 
# 
#         elapsed_time = time.time() - start_time
#         samples_per_second = (sample_num+1)/elapsed_time
# 
#         print("Epoch {} complete in in {:.2f} seconds, loss is {:.3E}. Samples per second {:.2f}".format(n_epoch+1, time.time() - start_time, loss/(sample_num+1), samples_per_second))
# 
#     return np.array(item_item_S), loss/(sample_num+1), samples_per_second
#

n_items = URM_train.shape[1]
learning_rate = 1e-3
regularization_2 = 1e-3

item_item_S, loss, samples_per_second = train_multiple_epochs(URM_train, learning_rate, regularization_2, 30)

item_item_S

URM_train_coo = URM_train.tocoo()

sample_index = np.random.randint(URM_train_coo.nnz)

user_id = URM_train_coo.row[sample_index]
item_id = URM_train_coo.col[sample_index]
true_rating = URM_train_coo.data[sample_index]

(user_id, item_id, true_rating)

predicted_rating = URM_train[user_id].dot(item_item_S[:,item_id])[0]

result = true_rating - predicted_rating
result

def filter_seen(URM, user_id, scores):

        start_pos = URM.indptr[user_id]
        end_pos = URM.indptr[user_id+1]

        user_profile = URM.indices[start_pos:end_pos]

        scores[user_profile] = -np.inf

        return scores

def recommend(URM, S,  user_id, at=10, exclude_seen=True):
        # compute the scores using the dot product
        user_profile = URM[user_id]
        scores = user_profile.dot(S).ravel()

        if exclude_seen:
            scores = filter_seen(URM, user_id, scores)

        # rank items
        ranking = scores.argsort()[::-1]

        return ranking[:at]

recommender = recommend(URM_train, item_item_S,1,10)
recommender

"""##****SUBMIT****"""

users_to_recommend_raw = pd.read_csv(filepath_or_buffer="/gdrive/My Drive/RecSys/libs/data_target_users_test.csv",
                   sep=",",
                  dtype={"user_id": np.int32}).to_numpy()
users_to_recommend = []
for elem in users_to_recommend_raw:
  users_to_recommend.append(elem[0])

len(users_to_recommend)

ratings = pd.read_csv("/gdrive/My Drive/RecSys/libs/data_train2.csv",
                       sep=",",
                       names=["user_id", "item_id", "ratings"],
                       header=None,
                       dtype={"user_id": np.int64,
                               "item_id": np.int64,
                               "ratings": np.int64})
ratings.shape

def preprocess_data(ratings: pd.DataFrame):
    unique_users = ratings.user_id.unique()
    unique_items = ratings.item_id.unique()

    num_users, min_user_id, max_user_id = unique_users.size, unique_users.min(), unique_users.max()
    num_items, min_item_id, max_item_id = unique_items.size, unique_items.min(), unique_items.max()

    print(num_users, min_user_id, max_user_id)
    print(num_items, min_item_id, max_item_id)

    mapping_user_id = pd.DataFrame({"mapped_user_id": np.arange(num_users), "user_id": unique_users})
    mapping_item_id = pd.DataFrame({"mapped_item_id": np.arange(num_items), "item_id": unique_items})

    ratings = pd.merge(left=ratings,
                       right=mapping_user_id,
                       how="inner",
                       on="user_id")

    ratings = pd.merge(left=ratings,
                       right=mapping_item_id,
                       how="inner",
                       on="item_id")

    return ratings

ratings = preprocess_data(ratings)

ratings.user_id

users_ids_and_mappings = ratings[ratings.user_id.isin(users_to_recommend)][["user_id", "mapped_user_id"]].drop_duplicates()
items_ids_and_mappings = ratings[["item_id", "mapped_item_id"]].drop_duplicates()
users_ids_and_mappings

def prepare_submission(ratings: pd.DataFrame, users_to_recommend: np.array, urm_train: sps.csr_matrix, recommender: object):
    users_ids_and_mappings = ratings[ratings.user_id.isin(users_to_recommend)][["user_id", "mapped_user_id"]].drop_duplicates()
    items_ids_and_mappings = ratings[["item_id", "mapped_item_id"]].drop_duplicates()

    mapping_to_item_id = dict(zip(ratings.mapped_item_id, ratings.item_id))


    recommendation_length = 10
    submission = []
    for idx, row in users_ids_and_mappings.iterrows():
        user_id = row.user_id
        mapped_user_id = row.mapped_user_id
        recommendations = []
        recommendations = recommend(URM_train,item_item_S, mapped_user_id,recommendation_length)
        submission.append((user_id, [mapping_to_item_id[item_id] for item_id in recommendations]))

    missing_users = []
    top10 = [ 517,  189,   44,    0,  284,  808,  285,  557,    1, 1266]
    for elem in users_to_recommend:
      miss = True
      for sub in submission:
        if elem == sub[0]:
          miss = False
      if miss:
        missing_users.append(elem)

      for i in range(len(missing_users)):
        missing_users[i] = (missing_users[i], [mapping_to_item_id[top10[elem]] for elem in range(10)])



    return submission + missing_users

recommender = recommend(URM_train, item_item_S,user_id,10)
submission = prepare_submission(ratings, users_to_recommend, URM_train, recommender)
len(submission)

submission

submission.sort()
for i in range(len(submission)):
  submission[i][1].sort()
submission

def write_submission(submissions):
    with open("./SLIMsubmission.csv", "w") as f:
        for user_id, items in submissions:
            f.write(f"{user_id},{' '.join([str(item) for item in items])}\n")

write_submission(submission)